{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0933e2",
   "metadata": {},
   "source": [
    "# Preprocessing of ArcticDEM, CryoSat-2 and Sentinel-1\n",
    "\n",
    "This script does all of the preprocessing of the elevation and Sentinel-1 data needed before training and applaying UDEM.\n",
    "\n",
    "Before you start running this script you need to download the following files from the original data sources:\n",
    "- The ArcticDEM mosaic (I have used the 100m version)\n",
    "- RGI7 of the region (RGI2000-v7.0-C-03_arctic_canada_north)\n",
    "- CryoSat2 swath measurements of your region (DOI to be added).\n",
    "\n",
    "\n",
    "My region of interest is the Northern Canadian Arctic, but for computational purposes I have sub-divided that region into 6 smaller ones.\n",
    "\n",
    "However, you can in principle do this for any region of interest as long as you have \n",
    "- A DEM and DEM strips (ArcticDEM or REMA for instance)\n",
    "- CryoSat-2 swath measurements\n",
    "- Sentinel-1 coverage\n",
    "- A shapefile (or any other geodata file which geopandas can read) of the region of interest\n",
    "\n",
    "\n",
    "Note! I have only tested the scripts on Mac (M4 Pro). I believe that they should work on all UNIX systems as well, but to work on Windows you need to adjust paths \"/\" to \"\\\\\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import os\n",
    "import preprocessing_tools as ptools\n",
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the project directory\n",
    "projectDir = \"/Users/rfk471/Dropbox/elevation-canada\"\n",
    "\n",
    "project_crs = \"EPSG:3413\"\n",
    "\n",
    "region_ids = [\"01\",\"02\",\"03\",\"04\",\"05\",\"06\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264054f1",
   "metadata": {},
   "source": [
    "## 0. Prepare ArcticDEM mosaic for the desire region\n",
    "\n",
    "Clip, reproject and save the ArcticDEM mosaic of the desired region to reduce computations later on.\n",
    "\n",
    "This part requires GDAL installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62654e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full path to the ArcticDEM mosaic\n",
    "demDir = f\"{projectDir}/data/initial/ArcticDEM/arcticdem_mosaic_100m_v4.1_dem.tif\"\n",
    "\n",
    "for region_id in region_ids:\n",
    "    region = geopandas.read_file(f\"{projectDir}/data/initial/regions/region-{region_id}.shp\")\n",
    "    ptools.clip_reproject_arcticdem(region,region_id,demDir,projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f6b93",
   "metadata": {},
   "source": [
    "## 1. Save seasonal CryoSat-2 tifs\n",
    "\n",
    "This part loads the CryoSat-2 swath files, cleans the data by excluding data points that differ more than 25 m from a reference DEM, and saves seasonal elevations by taking the median of the monthly elevations.\n",
    "\n",
    "The seasons are defined like this:\n",
    "Winter: Dec, Jan, Feb\n",
    "Spring: Mar, Apr, May\n",
    "Summer: Jun, Jul, Aug\n",
    "Fall: Sep, Oct, Nov\n",
    "\n",
    "The CryoSat-2 swath data used here has been downloaded from https://doi.org/10.4121/955d7f5a-0e3f-4166-a411-f0dcc4557cb2\n",
    "\n",
    "You can also do your own preprocessig of CryoSat-2 by using packages such as cryoswath (which was used for the data downloaded here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d590331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "dataDir = f\"{projectDir}/data/initial/cryoswath/\"\n",
    "\n",
    "for region_id in region_ids:\n",
    "\n",
    "    # Pick the right dataset depending on region\n",
    "    if region_id == \"01\":\n",
    "        CSfile = \"Glacier_surface_elevation__Arctic_Canada_North__Axel_Heiberg_and_Meighen_Is__monthly_500x500m.nc\"\n",
    "    elif region_id == \"02\":\n",
    "        CSfile = \"Glacier_surface_elevation__Arctic_Canada_North__North_Ellesmere_Island__monthly_500x500m.nc\"\n",
    "    elif region_id == \"03\":\n",
    "        CSfile = \"Glacier_surface_elevation__Arctic_Canada_North__North_Central_Ellesmere_Island__monthly_500x500m.nc\"\n",
    "    elif region_id == \"04\":\n",
    "        CSfile = \"Glacier_surface_elevation__Arctic_Canada_North__South_Central_Ellesmere_Island__monthly_500x500m.nc\"\n",
    "    elif region_id == \"05\":\n",
    "        CSfile = \"Glacier_surface_elevation__Arctic_Canada_North__South_Ellesmere_Island-Northwest_Devon__monthly_500x500m.nc\"\n",
    "    elif region_id == \"06\":\n",
    "        CSfile = \"Glacier_surface_elevation__Arctic_Canada_North__Devon_Island__monthly_500x500m.nc\"\n",
    "\n",
    "\n",
    "    ptools.save_seasonal_cryosat(region_id,dataDir,CSfile,projectDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81936d82",
   "metadata": {},
   "source": [
    "## 2. Download Sentinel-1\n",
    "\n",
    "This part requires a Google Earth Engine profile, and the Sentinel-1 files are downloaded to your Google Drive. \n",
    "\n",
    "The script computes and saves the seasonal means.\n",
    "\n",
    "!!! Be aware that if you, like me, don't have a lot of storage space in your Google Drive, you might need to start downloading the images before they have all been processed to not run into storage issues !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the below line if you haven't already authenticated your ee account\n",
    "\n",
    "#ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15624b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze your ee-project, remember to change the name to the name of your project\n",
    "ee.Initialize(project='bedcan')\n",
    "\n",
    "# Downloading everything might take some time... Be patient... Or at least try...\n",
    "for region_id in region_ids:\n",
    "    ptools.download_sentinel1(region_id,projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d749a",
   "metadata": {},
   "source": [
    "## 3. Downloading ArcticDEM strips\n",
    "\n",
    "This part takes quite a long time and has been put into a separate script.\n",
    "\n",
    "It downloads the 2m strips of a desired region (only the part of the region which intersect with the RGI shapefile of the region) in parallel, interpolates it onto a 50 m grid, and removes the original 2 m strips to save space.\n",
    "\n",
    "It requires the following data:\n",
    "- A shapefile of the region of interest\n",
    "- A shapefile of all strip indecies (e.g., ArcticDEM_Strip_Index_s2s041.shp, can be downloaded from https://www.pgc.umn.edu/data/arcticdem/)\n",
    "- The RGI v7 shapefile (to reduce number of strips to be downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29d9ac",
   "metadata": {},
   "source": [
    "I recommend to run the script directly from the terminal like this:\n",
    "\n",
    "`python download-arcticdem.py --region_id 02 --project_crs EPSG:3413 --n_threads 8 --projectDir \"/Users/rfk471/Dropbox/elevation-canada\"`\n",
    "\n",
    "Or like this to prevent your laptop (mac at least) to sleep:\n",
    "\n",
    "`caffeinate -i python download-arcticdem.py --region_id 02 --project_crs EPSG:3413 --n_threads 8 --projectDir \"/Users/rfk471/Dropbox/elevation-canada\"`\n",
    "\n",
    "\n",
    "\n",
    "`--region_id`: region_id as here, but you can also modify the script to your liking\n",
    "\n",
    "`--project_crs`: crs of the project, here we use Arctic Polar Stereographic\n",
    "\n",
    "`--n_threads`: Number of parallel threads to use (I have used 8 on a laptop with 14 available)\n",
    "\n",
    "`--projectDir`: The path to the projectDir just like defined here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd44c2",
   "metadata": {},
   "source": [
    "## 4. Create seasonal ArcticDEM tifs\n",
    "\n",
    "This part requires a few steps:\n",
    "1. Bit-masking: Mask out all non-good pixels based on the bit mask provided.\n",
    "2. Co-register the strips to the actual surface, by fitting a line through the residuals of the strip elevations and some reference elevation.\n",
    "3. Create seasonal means from the individual strips.\n",
    "\n",
    "\n",
    "Reference elevation:\n",
    "\n",
    "For the reference elevation I use the corresponding seasonal CryoSat-2 geotif in ice-covered areas and the ArcticDEM mosaic for ice-free areas. I use RGI v7 to determine whether a pixel is ice-covered or ice-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2934ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bit masking\n",
    "\n",
    "n_threads = 8 # Number of CPU cores to use\n",
    "\n",
    "for region_id in region_ids:\n",
    "    ptools.bit_masking(region_id,projectDir,n_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ce17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-registration\n",
    "\n",
    "n_threads = 2\n",
    "\n",
    "for region_id in region_ids:\n",
    "    ptools.co_registration(region_id,project_crs,projectDir,n_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d328ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal means\n",
    "\n",
    "for region_id in region_ids:\n",
    "    ptools.seasonal_arcticdem(region_id,projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e5e48",
   "metadata": {},
   "source": [
    "## 5. Regrid Sentinel-1\n",
    "\n",
    "Here, the Sentinel-1 files are regridded onto the same \"common\" grid as CryoSat-2 and ArcticDEM.\n",
    "\n",
    "Be aware that the Sentinel-1 files are assumed to be located here: `f\"{projectDir}/data/initial/sentinel-1/region-{region_id}/\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15163b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for region_id in region_ids:\n",
    "    ptools.regrid_sentinel1(region_id,projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e3808",
   "metadata": {},
   "source": [
    "## 6. Create icemask\n",
    "\n",
    "Create and save icemask based on the region and RGI v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_id in region_ids:\n",
    "    ptools.get_icemask(region_id, projectDir, project_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a2dcc",
   "metadata": {},
   "source": [
    "## 7. Create tile file\n",
    "\n",
    "This part creates a geopackage which defines the tiles used for the U-Net. The tiles are based on the 100 m mask. The script only generates tiles of ice-covered areas. The tiles are 64x64 pixels and have an 8 pixel overlap in all directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_id in region_ids:\n",
    "    ptools.create_tiles(region_id, projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0192f9b",
   "metadata": {},
   "source": [
    "## 8. Create netcdfs\n",
    "\n",
    "The point of this part is to generate 3 netcdf files with each of the three datasets as they are used in the U-Net model. \n",
    "\n",
    "All three datasets go through the following steps:\n",
    "1. Fill NaN pixels in a nearest neighbour manner in a maximum distance of 5 pixels of a data-filled pixel.\n",
    "2. (CryoSat-2 only) interpolate data onto the high-resolution grid (same grid as S-1 and ArcticDEM).\n",
    "3. Apply the icemask to mask out pixels in ice-free areas and set them to 0, and treat 0 as a NO DATA value.\n",
    "4. Concatenate all dataframes along a time dimension and save to netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for region_id in region_ids:\n",
    "    ptools.create_netcdfs(region_id,projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9791c",
   "metadata": {},
   "source": [
    "## 9. Normalize, tile, and split data\n",
    "\n",
    "Nomalization: You have the choice to normalize the data or not. The valid options are `\"raw\"` (no normalization) and `\"zscore\"` (all data is normalized in a zscore manner).\n",
    "\n",
    "Tiling: All data is clipped into smaller tiles using the predefined tiles. The tiled data is saved in a hdf5 file with the following structure:\n",
    "\n",
    "```\n",
    "seasonal_tiles_normalization_{normalization}.h5\n",
    "├── 2014_10\n",
    "│   ├── tile_001\n",
    "│   │   ├── cs_tile\n",
    "│   │   ├── s1_tile\n",
    "│   │   ├── adem_tile\n",
    "│   │   ├── mask_tile\n",
    "│   │   └── (attrs: cs_flag, s1_flag, adem_flag)\n",
    "│   ├── tile_002\n",
    "│   │   ├── ...\n",
    "│   └── ...\n",
    "├── 2015_01\n",
    "│   └── ...\n",
    "└── ...\n",
    "```\n",
    "\n",
    "Data splitting:\n",
    "\n",
    "All tiles where S1 and CryoSat2 have full coverage are put into the `all_tiles` dataframe and saved as `X_all.pkl`. This is the data which we eventually want to predict on once we have a working U-Net.\n",
    "\n",
    "All tiles where all three datasets have full coverage are put into the `complete_tiles` dataframe. This data is further split into training data (80%), test data (10%) and validation data (10%). HPO will be performed on the validation dataset on which we do a 80/20 split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = \"raw\"\n",
    "for region_id in region_ids:\n",
    "    ptools.normalize_tiling(region_id,projectDir,project_crs,normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc472bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_tiles, all_tiles = ptools.load_complete_tiles(region_ids,normalization,projectDir)\n",
    "\n",
    "random_number = 7\n",
    "\n",
    "ptools.split_save_data(complete_tiles,all_tiles,projectDir,normalization,random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are in total {len(complete_tiles)} tiles with full coverage of all datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5ec3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elevation-canada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
