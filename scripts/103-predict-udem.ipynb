{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "921497cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "148c535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, df_X):\n",
    "        self.df_X = df_X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --- Inputs ---\n",
    "        s1 = self.df_X.iloc[idx][\"s1\"]   # numpy array (H, W)\n",
    "        cs = self.df_X.iloc[idx][\"cs\"]   # numpy array (H, W)\n",
    "\n",
    "        # Convert to tensor\n",
    "        s1 = torch.tensor(s1, dtype=torch.float32)\n",
    "        cs = torch.tensor(cs, dtype=torch.float32)\n",
    "\n",
    "        # Stack into shape (2, H, W)\n",
    "        X = torch.stack([s1, cs], dim=0)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65ba6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = \"raw\"\n",
    "dataDir = f\"../data/interim/normalization-{normalization}/\"\n",
    "\n",
    "X_all = pd.read_pickle(f'{dataDir}/X_all.pkl')\n",
    "\n",
    "prediction_dataset = PredictionDataset(X_all)\n",
    "prediction_loader = DataLoader(prediction_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9bf7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conv Block ---\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batchnorm=True, dropout_rate=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)]\n",
    "        if batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        if batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "# --- Encoder Block ---\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_conv = self.conv(x)\n",
    "        x_pooled = self.pool(x_conv)\n",
    "        return x_conv, x_pooled\n",
    "    \n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.wg = nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0)\n",
    "        self.wx = nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0)\n",
    "        self.psi = nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.wg(g)\n",
    "        x1 = self.wx(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.sigmoid(self.psi(psi))\n",
    "        return x * psi\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# --- U-Net ---\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1, int_filters = 32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        df = int_filters\n",
    "        uf = int_filters\n",
    "\n",
    "        self.e1 = EncoderBlock(in_channels, df)\n",
    "        self.e2 = EncoderBlock(df, df * 2)\n",
    "        self.e3 = EncoderBlock(df * 2, df * 4)\n",
    "        self.e4 = EncoderBlock(df * 4, df * 8)\n",
    "\n",
    "        self.bottleneck = ConvBlock(df * 8, df * 16)\n",
    "\n",
    "        self.up4 = UpConv(df * 16, uf * 8)\n",
    "        self.att4 = AttentionBlock(uf * 8, df * 8, uf * 8)\n",
    "        self.conv4 = ConvBlock(uf * 8 + df * 8, uf * 8)\n",
    "\n",
    "        self.up3 = UpConv(uf * 8, uf * 4)\n",
    "        self.att3 = AttentionBlock(uf * 4, df * 4, uf * 4)\n",
    "        self.conv3 = ConvBlock(uf * 4 + df * 4, uf * 4)\n",
    "\n",
    "        self.up2 = UpConv(uf * 4, uf * 2)\n",
    "        self.att2 = AttentionBlock(uf * 2, df * 2, uf * 2)\n",
    "        self.conv2 = ConvBlock(uf * 2 + df * 2, uf * 2)\n",
    "\n",
    "        self.up1 = UpConv(uf * 2, uf)\n",
    "        self.att1 = AttentionBlock(uf, df, uf)\n",
    "        self.conv1 = ConvBlock(uf + df, uf)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(uf, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1, p1 = self.e1(x)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        b = self.bottleneck(p4)\n",
    "\n",
    "        u4 = self.up4(b)\n",
    "        a4 = self.att4(u4, s4)\n",
    "        u4 = torch.cat([u4, a4], dim=1)\n",
    "        c4 = self.conv4(u4)\n",
    "\n",
    "        u3 = self.up3(c4)\n",
    "        a3 = self.att3(u3, s3)\n",
    "        u3 = torch.cat([u3, a3], dim=1)\n",
    "        c3 = self.conv3(u3)\n",
    "\n",
    "        u2 = self.up2(c3)\n",
    "        a2 = self.att2(u2, s2)\n",
    "        u2 = torch.cat([u2, a2], dim=1)\n",
    "        c2 = self.conv2(u2)\n",
    "\n",
    "        u1 = self.up1(c2)\n",
    "        a1 = self.att1(u1, s1)\n",
    "        u1 = torch.cat([u1, a1], dim=1)\n",
    "        c1 = self.conv1(u1)\n",
    "\n",
    "        outputs = self.final_conv(c1)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f65e58a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e1): EncoderBlock(\n",
       "    (conv): ConvBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "      (dropout): Identity()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e2): EncoderBlock(\n",
       "    (conv): ConvBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "      (dropout): Identity()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e3): EncoderBlock(\n",
       "    (conv): ConvBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "      (dropout): Identity()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e4): EncoderBlock(\n",
       "    (conv): ConvBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "      (dropout): Identity()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bottleneck): ConvBlock(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (dropout): Identity()\n",
       "  )\n",
       "  (up4): UpConv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (att4): AttentionBlock(\n",
       "    (wg): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (wx): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (psi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (dropout): Identity()\n",
       "  )\n",
       "  (up3): UpConv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (att3): AttentionBlock(\n",
       "    (wg): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (wx): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (psi): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (dropout): Identity()\n",
       "  )\n",
       "  (up2): UpConv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (att2): AttentionBlock(\n",
       "    (wg): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (wx): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (psi): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (dropout): Identity()\n",
       "  )\n",
       "  (up1): UpConv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (att1): AttentionBlock(\n",
       "    (wg): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (wx): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (psi): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (dropout): Identity()\n",
       "  )\n",
       "  (final_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(in_channels=2, out_channels=1, int_filters=32)\n",
    "model.load_state_dict(torch.load(f\"../models/unet_normalization_{normalization}.pth\", weights_only=True))\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40932993",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X in prediction_loader:\n",
    "        X = X.to(device)\n",
    "\n",
    "        preds = model(X)              # (B, 1, H, W)\n",
    "        preds = preds.cpu().numpy()   # convert to numpy\n",
    "\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fafb8733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25384"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9db57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc1f7bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203070, 64, 64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[:,0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adac460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all['pred'] = list(all_preds[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba4a3400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cs</th>\n",
       "      <th>s1</th>\n",
       "      <th>tile</th>\n",
       "      <th>region</th>\n",
       "      <th>time</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1321.5018, 1321.5018, 1321.5018, 1321.5018, ...</td>\n",
       "      <td>[[-3.0440319, -3.9007003, -4.656244, -6.187236...</td>\n",
       "      <td>tile_10</td>\n",
       "      <td>01</td>\n",
       "      <td>2014_10</td>\n",
       "      <td>[[1227.6663, 1277.6422, 1297.5355, 1279.4984, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 888.5931, 888.5931, 888....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, -0.46023303, -7.0914674,...</td>\n",
       "      <td>tile_101</td>\n",
       "      <td>01</td>\n",
       "      <td>2014_10</td>\n",
       "      <td>[[9.129043, 8.786194, 9.184349, 8.947771, 885....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1294.3636, 1294.3636, 1294.3636, 1350.764, 1...</td>\n",
       "      <td>[[-0.48107538, -1.604826, -2.2076383, -2.89677...</td>\n",
       "      <td>tile_108</td>\n",
       "      <td>01</td>\n",
       "      <td>2014_10</td>\n",
       "      <td>[[1164.9523, 1239.4497, 1302.0836, 1303.2202, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[186.76949, 186.76949, 186.76949, 186.76949, ...</td>\n",
       "      <td>[[-8.337175, -8.040663, -9.163354, -9.161297, ...</td>\n",
       "      <td>tile_109</td>\n",
       "      <td>01</td>\n",
       "      <td>2014_10</td>\n",
       "      <td>[[176.29524, 201.40758, 183.69368, 186.56265, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[895.30536, 895.30536, 895.30536, 895.30536, ...</td>\n",
       "      <td>[[-5.05783, -5.5084124, -5.58629, -5.8013773, ...</td>\n",
       "      <td>tile_11</td>\n",
       "      <td>01</td>\n",
       "      <td>2014_10</td>\n",
       "      <td>[[860.23895, 870.6348, 859.527, 862.0249, 861....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203065</th>\n",
       "      <td>[[1105.7786, 1105.7786, 1105.7786, 1105.7786, ...</td>\n",
       "      <td>[[-4.825405, -4.721585, -4.73615, -4.424411, -...</td>\n",
       "      <td>tile_95</td>\n",
       "      <td>06</td>\n",
       "      <td>2024_10</td>\n",
       "      <td>[[1045.689, 1051.1731, 1045.0349, 1053.5786, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203066</th>\n",
       "      <td>[[1252.2408, 1252.2408, 1257.4795, 1257.4795, ...</td>\n",
       "      <td>[[-3.1178699, -3.0993083, -3.2134385, -3.48768...</td>\n",
       "      <td>tile_96</td>\n",
       "      <td>06</td>\n",
       "      <td>2024_10</td>\n",
       "      <td>[[1171.3232, 1186.3425, 1179.0221, 1182.1045, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203067</th>\n",
       "      <td>[[1324.8103, 1324.8103, 1324.8103, 1324.8103, ...</td>\n",
       "      <td>[[-2.9030392, -2.9227698, -3.2303104, -3.27568...</td>\n",
       "      <td>tile_97</td>\n",
       "      <td>06</td>\n",
       "      <td>2024_10</td>\n",
       "      <td>[[1227.8917, 1243.9807, 1236.8665, 1234.3662, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203068</th>\n",
       "      <td>[[1148.0347, 1148.0347, 1156.8484, 1156.8484, ...</td>\n",
       "      <td>[[-6.92509, -6.6605544, -6.6482167, -6.5667467...</td>\n",
       "      <td>tile_98</td>\n",
       "      <td>06</td>\n",
       "      <td>2024_10</td>\n",
       "      <td>[[1082.5204, 1081.5443, 1078.659, 1087.716, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203069</th>\n",
       "      <td>[[1073.9655, 1073.9655, 1084.8396, 1084.8396, ...</td>\n",
       "      <td>[[-6.042366, -5.8604226, -5.871729, -5.8064528...</td>\n",
       "      <td>tile_99</td>\n",
       "      <td>06</td>\n",
       "      <td>2024_10</td>\n",
       "      <td>[[1018.9855, 1023.1824, 1019.8388, 1028.233, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203070 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       cs  \\\n",
       "0       [[1321.5018, 1321.5018, 1321.5018, 1321.5018, ...   \n",
       "1       [[0.0, 0.0, 0.0, 0.0, 888.5931, 888.5931, 888....   \n",
       "2       [[1294.3636, 1294.3636, 1294.3636, 1350.764, 1...   \n",
       "3       [[186.76949, 186.76949, 186.76949, 186.76949, ...   \n",
       "4       [[895.30536, 895.30536, 895.30536, 895.30536, ...   \n",
       "...                                                   ...   \n",
       "203065  [[1105.7786, 1105.7786, 1105.7786, 1105.7786, ...   \n",
       "203066  [[1252.2408, 1252.2408, 1257.4795, 1257.4795, ...   \n",
       "203067  [[1324.8103, 1324.8103, 1324.8103, 1324.8103, ...   \n",
       "203068  [[1148.0347, 1148.0347, 1156.8484, 1156.8484, ...   \n",
       "203069  [[1073.9655, 1073.9655, 1084.8396, 1084.8396, ...   \n",
       "\n",
       "                                                       s1      tile region  \\\n",
       "0       [[-3.0440319, -3.9007003, -4.656244, -6.187236...   tile_10     01   \n",
       "1       [[0.0, 0.0, 0.0, 0.0, -0.46023303, -7.0914674,...  tile_101     01   \n",
       "2       [[-0.48107538, -1.604826, -2.2076383, -2.89677...  tile_108     01   \n",
       "3       [[-8.337175, -8.040663, -9.163354, -9.161297, ...  tile_109     01   \n",
       "4       [[-5.05783, -5.5084124, -5.58629, -5.8013773, ...   tile_11     01   \n",
       "...                                                   ...       ...    ...   \n",
       "203065  [[-4.825405, -4.721585, -4.73615, -4.424411, -...   tile_95     06   \n",
       "203066  [[-3.1178699, -3.0993083, -3.2134385, -3.48768...   tile_96     06   \n",
       "203067  [[-2.9030392, -2.9227698, -3.2303104, -3.27568...   tile_97     06   \n",
       "203068  [[-6.92509, -6.6605544, -6.6482167, -6.5667467...   tile_98     06   \n",
       "203069  [[-6.042366, -5.8604226, -5.871729, -5.8064528...   tile_99     06   \n",
       "\n",
       "           time                                               pred  \n",
       "0       2014_10  [[1227.6663, 1277.6422, 1297.5355, 1279.4984, ...  \n",
       "1       2014_10  [[9.129043, 8.786194, 9.184349, 8.947771, 885....  \n",
       "2       2014_10  [[1164.9523, 1239.4497, 1302.0836, 1303.2202, ...  \n",
       "3       2014_10  [[176.29524, 201.40758, 183.69368, 186.56265, ...  \n",
       "4       2014_10  [[860.23895, 870.6348, 859.527, 862.0249, 861....  \n",
       "...         ...                                                ...  \n",
       "203065  2024_10  [[1045.689, 1051.1731, 1045.0349, 1053.5786, 1...  \n",
       "203066  2024_10  [[1171.3232, 1186.3425, 1179.0221, 1182.1045, ...  \n",
       "203067  2024_10  [[1227.8917, 1243.9807, 1236.8665, 1234.3662, ...  \n",
       "203068  2024_10  [[1082.5204, 1081.5443, 1078.659, 1087.716, 10...  \n",
       "203069  2024_10  [[1018.9855, 1023.1824, 1019.8388, 1028.233, 1...  \n",
       "\n",
       "[203070 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e281a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
